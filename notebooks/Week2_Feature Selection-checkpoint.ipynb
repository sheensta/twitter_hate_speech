{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Hate Speech Analysis - Week 2\n",
    "## Colin Green, Sean Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sqlite3\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import previous df from sqlite\n",
    "con = sqlite3.connect('twitter_hate.db')\n",
    "sql = \"\"\"\n",
    "SELECT * FROM tweets_nlp\n",
    "\"\"\"\n",
    "with sqlite3.connect('twitter_hate.db') as con:\n",
    "    df = pd.read_sql_query(sql, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "tweets = df['tweet_clean']\n",
    "\n",
    "mentions = []\n",
    "urls = []\n",
    "hashtags = []\n",
    "i = 0\n",
    "for tweet in tweets:\n",
    "    tweet = tweet.split()\n",
    "    mentions.append(tweet.count('mentionhere')+tweet.count('mentionhere:')+tweet.count('\"mentionhere:')+tweet.count('&#;mentionhere:'))\n",
    "    urls.append(tweet.count('urlhere'))\n",
    "    hashtags.append(tweet.count('hashtaghere'))\n",
    "    tweet = [token for token in tweet if token not in [';&','']]\n",
    "    tweet = [token for token in tweet if token not in ['&#;mentionhere:','mentionhere:','\"mentionhere:','mentionhere', 'urlhere', 'hashtaghere', 'rt', 'amp']]\n",
    "    tweet = \" \".join(tweet)\n",
    "    tweets[i] = tweet\n",
    "    i += 1\n",
    "    \n",
    "df['tweet_no_others'] = tweets\n",
    "df['mention_count'] = mentions\n",
    "df['url_count'] = urls\n",
    "df['hashtag_count'] = hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_nouns</th>\n",
       "      <th>tweet_sym</th>\n",
       "      <th>tweet_verbs</th>\n",
       "      <th>tweet_nv</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>tweet_no_others</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" bitch who do you love \"</td>\n",
       "      <td>bitch who do you love</td>\n",
       "      <td>bitch who do -PRON- love</td>\n",
       "      <td>bitch</td>\n",
       "      <td></td>\n",
       "      <td>love</td>\n",
       "      <td>bitch love</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bitch who do you love</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" fuck no that bitch dont even suck dick \" &amp;#1...</td>\n",
       "      <td>fuck no that bitch dont even suck dick the ker...</td>\n",
       "      <td>fuck no that bitch do not even suck dick      ...</td>\n",
       "      <td>bitch dick kermit video bout</td>\n",
       "      <td></td>\n",
       "      <td>suck fuck</td>\n",
       "      <td>bitch dick kermit video bout suck fuck</td>\n",
       "      <td>18.0</td>\n",
       "      <td>fuck no that bitch dont even suck dick the ker...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\" lames crying over hoes thats tears of a clown \"</td>\n",
       "      <td>lames crying over hoes thats tears of a clown</td>\n",
       "      <td>lame cry over hoe that s tear of a clown</td>\n",
       "      <td>lame hoe tear clown</td>\n",
       "      <td></td>\n",
       "      <td>cry s</td>\n",
       "      <td>lame hoe tear clown cry s</td>\n",
       "      <td>10.0</td>\n",
       "      <td>lames crying over hoes thats tears of a clown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"..All I wanna do is get money and fuck model ...</td>\n",
       "      <td>all i wanna do is get money and fuck model bit...</td>\n",
       "      <td>all i wanna do be get money and fuck model bit...</td>\n",
       "      <td>wanna money fuck model bitch russell simmons</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>wanna money fuck model bitch russell simmons</td>\n",
       "      <td>14.0</td>\n",
       "      <td>all i wanna do is get money and fuck model bit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"@ARIZZLEINDACUT: Females think dating a pussy...</td>\n",
       "      <td>females think dating a pussy is cute now how d...</td>\n",
       "      <td>mentionhere   female think date a pussy be cut...</td>\n",
       "      <td>mentionhere female pussy stuff pussy</td>\n",
       "      <td></td>\n",
       "      <td>think date urlhere do make</td>\n",
       "      <td>mentionhere female pussy stuff pussy think dat...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>females think dating a pussy is cute now how d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  count  hate_speech  offensive_language  neither  class  \\\n",
       "0        0     17      3            1                   2        0      1   \n",
       "1        1     23      3            0                   3        0      1   \n",
       "2        2     38      3            0                   2        1      1   \n",
       "3        3     59      3            0                   3        0      1   \n",
       "4        4     62      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  \\\n",
       "0                          \" bitch who do you love \"   \n",
       "1  \" fuck no that bitch dont even suck dick \" &#1...   \n",
       "2  \" lames crying over hoes thats tears of a clown \"   \n",
       "3  \"..All I wanna do is get money and fuck model ...   \n",
       "4  \"@ARIZZLEINDACUT: Females think dating a pussy...   \n",
       "\n",
       "                                         tweet_clean  \\\n",
       "0                              bitch who do you love   \n",
       "1  fuck no that bitch dont even suck dick the ker...   \n",
       "2      lames crying over hoes thats tears of a clown   \n",
       "3  all i wanna do is get money and fuck model bit...   \n",
       "4  females think dating a pussy is cute now how d...   \n",
       "\n",
       "                                         tweet_lemma  \\\n",
       "0                           bitch who do -PRON- love   \n",
       "1  fuck no that bitch do not even suck dick      ...   \n",
       "2           lame cry over hoe that s tear of a clown   \n",
       "3  all i wanna do be get money and fuck model bit...   \n",
       "4  mentionhere   female think date a pussy be cut...   \n",
       "\n",
       "                                    tweet_nouns tweet_sym  \\\n",
       "0                                         bitch             \n",
       "1                  bitch dick kermit video bout             \n",
       "2                           lame hoe tear clown             \n",
       "3  wanna money fuck model bitch russell simmons             \n",
       "4          mentionhere female pussy stuff pussy             \n",
       "\n",
       "                  tweet_verbs  \\\n",
       "0                        love   \n",
       "1                   suck fuck   \n",
       "2                       cry s   \n",
       "3                               \n",
       "4  think date urlhere do make   \n",
       "\n",
       "                                            tweet_nv  num_tokens  \\\n",
       "0                                         bitch love         5.0   \n",
       "1             bitch dick kermit video bout suck fuck        18.0   \n",
       "2                          lame hoe tear clown cry s        10.0   \n",
       "3       wanna money fuck model bitch russell simmons        14.0   \n",
       "4  mentionhere female pussy stuff pussy think dat...        22.0   \n",
       "\n",
       "                                     tweet_no_others  mention_count  \\\n",
       "0                              bitch who do you love              0   \n",
       "1  fuck no that bitch dont even suck dick the ker...              0   \n",
       "2      lames crying over hoes thats tears of a clown              0   \n",
       "3  all i wanna do is get money and fuck model bit...              0   \n",
       "4  females think dating a pussy is cute now how d...              1   \n",
       "\n",
       "   url_count  hashtag_count  \n",
       "0          0              0  \n",
       "1          0              0  \n",
       "2          0              0  \n",
       "3          0              0  \n",
       "4          1              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['tweet_no_others']\n",
    "corpus = corpus.str.replace('hashtaghere', '')\n",
    "corpus = corpus.str.replace('urlhere', '')\n",
    "corpus = corpus.str.replace('mentionhere', '')\n",
    "corpus = corpus.str.replace('rt', '')\n",
    "corpus = corpus.str.replace(';&', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5177\n",
      "Vocabulary Sample: [('a', 1), ('i', 2), ('you', 3), ('bitch', 4), ('the', 5), ('to', 6), ('that', 7), ('and', 8), ('t', 9), ('is', 10)]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['bitch', 'who', 'you', 'love'] -> Target (Y): do\n",
      "Context (X): ['fuck', 'no', 'bitch', 'dont'] -> Target (Y): that\n",
      "Context (X): ['no', 'that', 'dont', 'even'] -> Target (Y): bitch\n",
      "Context (X): ['that', 'bitch', 'even', 'suck'] -> Target (Y): dont\n",
      "Context (X): ['bitch', 'dont', 'suck', 'dick'] -> Target (Y): even\n",
      "Context (X): ['dont', 'even', 'dick', 'the'] -> Target (Y): suck\n",
      "Context (X): ['even', 'suck', 'the', 'kermit'] -> Target (Y): dick\n",
      "Context (X): ['suck', 'dick', 'kermit', 'videos'] -> Target (Y): the\n",
      "Context (X): ['dick', 'the', 'videos', 'bout'] -> Target (Y): kermit\n",
      "Context (X): ['the', 'kermit', 'bout', 'to'] -> Target (Y): videos\n",
      "Context (X): ['kermit', 'videos', 'to', 'fuck'] -> Target (Y): bout\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            517700    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5177)              522877    \n",
      "=================================================================\n",
      "Total params: 1,040,577\n",
      "Trainable params: 1,040,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"405pt\" viewBox=\"0.00 0.00 242.00 304.00\" width=\"323pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 238,-300 238,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2680432703048 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2680432703048</title>\n",
       "<polygon fill=\"none\" points=\"15.5,-249.5 15.5,-295.5 218.5,-295.5 218.5,-249.5 15.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-268.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"92.5,-249.5 92.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"92.5,-272.5 148.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"148.5,-249.5 148.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-280.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"148.5,-272.5 218.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-257.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 2680259013384 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2680259013384</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 234,-212.5 234,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40\" y=\"-185.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"80,-166.5 80,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"80,-189.5 136,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"136,-166.5 136,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-197.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"136,-189.5 234,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185\" y=\"-174.3\">(None, 4, 100)</text>\n",
       "</g>\n",
       "<!-- 2680432703048&#45;&gt;2680259013384 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2680432703048-&gt;2680259013384</title>\n",
       "<path d=\"M117,-249.366C117,-241.152 117,-231.658 117,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"120.5,-222.607 117,-212.607 113.5,-222.607 120.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680302558216 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2680302558216</title>\n",
       "<polygon fill=\"none\" points=\"9,-83.5 9,-129.5 225,-129.5 225,-83.5 9,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40\" y=\"-102.8\">Lambda</text>\n",
       "<polyline fill=\"none\" points=\"71,-83.5 71,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"71,-106.5 127,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"127,-83.5 127,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176\" y=\"-114.3\">(None, 4, 100)</text>\n",
       "<polyline fill=\"none\" points=\"127,-106.5 225,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 2680259013384&#45;&gt;2680302558216 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2680259013384-&gt;2680302558216</title>\n",
       "<path d=\"M117,-166.366C117,-158.152 117,-148.658 117,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"120.5,-139.607 117,-129.607 113.5,-139.607 120.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680434173512 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2680434173512</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-0.5 18.5,-46.5 215.5,-46.5 215.5,-0.5 18.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"44\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"69.5,-0.5 69.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"69.5,-23.5 125.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"125.5,-0.5 125.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"125.5,-23.5 215.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-8.3\">(None, 5177)</text>\n",
       "</g>\n",
       "<!-- 2680302558216&#45;&gt;2680434173512 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2680302558216-&gt;2680434173512</title>\n",
       "<path d=\"M117,-83.3664C117,-75.1516 117,-65.6579 117,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"120.5,-56.6068 117,-46.6068 113.5,-56.6069 120.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch: 1 \tLoss: 269066.11168396473\n",
      "\n",
      "Epoch: 2 \tLoss: 316429.53104421124\n",
      "\n",
      "Epoch: 3 \tLoss: 328783.44251821283\n",
      "\n",
      "Epoch: 4 \tLoss: 343660.9442778947\n",
      "\n",
      "Epoch: 5 \tLoss: 359828.0662963758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5176, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>-0.344456</td>\n",
       "      <td>0.292898</td>\n",
       "      <td>-0.262916</td>\n",
       "      <td>0.646709</td>\n",
       "      <td>0.302199</td>\n",
       "      <td>-0.828447</td>\n",
       "      <td>-0.343131</td>\n",
       "      <td>0.517349</td>\n",
       "      <td>0.320776</td>\n",
       "      <td>0.331731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939612</td>\n",
       "      <td>-0.130678</td>\n",
       "      <td>-0.170145</td>\n",
       "      <td>-0.115964</td>\n",
       "      <td>-0.392739</td>\n",
       "      <td>0.458267</td>\n",
       "      <td>0.495998</td>\n",
       "      <td>-0.166956</td>\n",
       "      <td>-0.283363</td>\n",
       "      <td>0.654723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.240395</td>\n",
       "      <td>0.307432</td>\n",
       "      <td>-0.340946</td>\n",
       "      <td>0.511186</td>\n",
       "      <td>0.733332</td>\n",
       "      <td>-0.120478</td>\n",
       "      <td>-0.658155</td>\n",
       "      <td>0.351165</td>\n",
       "      <td>0.703330</td>\n",
       "      <td>0.901953</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.524077</td>\n",
       "      <td>-0.499254</td>\n",
       "      <td>-0.695410</td>\n",
       "      <td>-0.017812</td>\n",
       "      <td>-0.248006</td>\n",
       "      <td>0.512228</td>\n",
       "      <td>0.614553</td>\n",
       "      <td>-0.221892</td>\n",
       "      <td>-0.040281</td>\n",
       "      <td>0.377412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitch</th>\n",
       "      <td>-0.158742</td>\n",
       "      <td>0.324612</td>\n",
       "      <td>-0.338476</td>\n",
       "      <td>0.157366</td>\n",
       "      <td>0.137304</td>\n",
       "      <td>-0.087443</td>\n",
       "      <td>-0.590936</td>\n",
       "      <td>0.408860</td>\n",
       "      <td>0.766948</td>\n",
       "      <td>0.187936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726056</td>\n",
       "      <td>-0.553552</td>\n",
       "      <td>-0.279532</td>\n",
       "      <td>-0.124537</td>\n",
       "      <td>-0.475730</td>\n",
       "      <td>0.348130</td>\n",
       "      <td>0.568879</td>\n",
       "      <td>-0.055950</td>\n",
       "      <td>-0.398942</td>\n",
       "      <td>0.188298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.217844</td>\n",
       "      <td>0.261579</td>\n",
       "      <td>-0.063161</td>\n",
       "      <td>0.082009</td>\n",
       "      <td>0.069571</td>\n",
       "      <td>-0.605802</td>\n",
       "      <td>0.256339</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.458932</td>\n",
       "      <td>0.259504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>-0.335742</td>\n",
       "      <td>0.115323</td>\n",
       "      <td>0.050557</td>\n",
       "      <td>-0.289795</td>\n",
       "      <td>0.255858</td>\n",
       "      <td>-0.030456</td>\n",
       "      <td>-0.360857</td>\n",
       "      <td>-0.041826</td>\n",
       "      <td>0.254129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.544202</td>\n",
       "      <td>0.428513</td>\n",
       "      <td>-0.340148</td>\n",
       "      <td>0.515842</td>\n",
       "      <td>0.952850</td>\n",
       "      <td>-0.553234</td>\n",
       "      <td>-0.816141</td>\n",
       "      <td>0.299655</td>\n",
       "      <td>0.309088</td>\n",
       "      <td>0.406806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381888</td>\n",
       "      <td>-0.197808</td>\n",
       "      <td>-0.460100</td>\n",
       "      <td>0.094755</td>\n",
       "      <td>-0.374914</td>\n",
       "      <td>0.649742</td>\n",
       "      <td>0.602346</td>\n",
       "      <td>-0.225918</td>\n",
       "      <td>0.066103</td>\n",
       "      <td>0.939461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "i     -0.344456  0.292898 -0.262916  0.646709  0.302199 -0.828447 -0.343131   \n",
       "you    0.240395  0.307432 -0.340946  0.511186  0.733332 -0.120478 -0.658155   \n",
       "bitch -0.158742  0.324612 -0.338476  0.157366  0.137304 -0.087443 -0.590936   \n",
       "the   -0.217844  0.261579 -0.063161  0.082009  0.069571 -0.605802  0.256339   \n",
       "to    -0.544202  0.428513 -0.340148  0.515842  0.952850 -0.553234 -0.816141   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "i      0.517349  0.320776  0.331731  ... -0.939612 -0.130678 -0.170145   \n",
       "you    0.351165  0.703330  0.901953  ... -1.524077 -0.499254 -0.695410   \n",
       "bitch  0.408860  0.766948  0.187936  ... -0.726056 -0.553552 -0.279532   \n",
       "the    0.004929  0.458932  0.259504  ...  0.003114 -0.335742  0.115323   \n",
       "to     0.299655  0.309088  0.406806  ... -0.381888 -0.197808 -0.460100   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "i     -0.115964 -0.392739  0.458267  0.495998 -0.166956 -0.283363  0.654723  \n",
       "you   -0.017812 -0.248006  0.512228  0.614553 -0.221892 -0.040281  0.377412  \n",
       "bitch -0.124537 -0.475730  0.348130  0.568879 -0.055950 -0.398942  0.188298  \n",
       "the    0.050557 -0.289795  0.255858 -0.030456 -0.360857 -0.041826  0.254129  \n",
       "to     0.094755 -0.374914  0.649742  0.602346 -0.225918  0.066103  0.939461  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5176, 5176)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hate': ['stay', 'am', 'mean', 'aint', 'tho'],\n",
       " 'love': ['new', 'wit', 'every', 'never', 'than'],\n",
       " 'nigger': ['monkey', 'damn', 'black', 'were', 'racist'],\n",
       " 'faggot': ['nigger', 'ur', 'faggots', 'damn', 'being'],\n",
       " 'bitch': ['her', 'pussy', 'he', 'shit', 'hoe'],\n",
       " 'pussy': ['faggots', 'her', 'niggas', 'one', 'girls'],\n",
       " 'cracker': ['negro', 'redneck', 'making', 'dem', 'monkey'],\n",
       " 'nigga': ['niggas', 'man', 'its', 'make', 'yo'],\n",
       " 'homo': ['homie', 'hop', 'muzzie', 'killed', 'j'],\n",
       " 'cunt': ['making', 'monkey', 'nicca', 'queer', 'shot'],\n",
       " 'fuck': ['do', 'what', 'smh', 'stupid', 'music'],\n",
       " 'trash': ['people', 'faggots', 'who', 'how', 'so'],\n",
       " 'queer': ['making', 'gets', 'boy', 'redneck', 'video']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['hate','love','nigger','faggot','bitch','pussy','cracker','nigga','homo','cunt','fuck','trash','queer']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5177\n",
      "Vocabulary Sample: [('a', 1), ('i', 2), ('you', 3), ('bitch', 4), ('the', 5), ('to', 6), ('that', 7), ('and', 8), ('t', 9), ('is', 10)]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(you (3), do (58)) -> 1\n",
      "(do (58), faggot (31)) -> 0\n",
      "(who (76), calling (394)) -> 0\n",
      "(you (3), looooool (4063)) -> 0\n",
      "(you (3), bitch (4)) -> 1\n",
      "(love (74), tuck (4424)) -> 0\n",
      "(do (58), you (3)) -> 1\n",
      "(who (76), do (58)) -> 1\n",
      "(love (74), you (3)) -> 1\n",
      "(who (76), you (3)) -> 1\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       517700      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100, 1)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100, 1)       0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 517,702\n",
      "Trainable params: 517,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanx\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers import dot\n",
    "\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, embed_size, input_length=1, name='embedding')\n",
    "\n",
    "word_embedding = embedding(input_target)\n",
    "word_embedding = Reshape((embed_size, 1))(word_embedding)\n",
    "context_embedding = embedding(input_context)\n",
    "context_embedding = Reshape((embed_size, 1))(context_embedding)\n",
    "\n",
    "# now perform the dot product operation  \n",
    "dot_product = dot([word_embedding, context_embedding], axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"627pt\" viewBox=\"0.00 0.00 464.00 470.00\" width=\"619pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-466 460,-466 460,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2680433553032 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2680433553032</title>\n",
       "<polygon fill=\"none\" points=\"16,-415.5 16,-461.5 219,-461.5 219,-415.5 16,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54.5\" y=\"-434.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"93,-415.5 93,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"93,-438.5 149,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"149,-415.5 149,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184\" y=\"-446.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"149,-438.5 219,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184\" y=\"-423.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2680738132872 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2680738132872</title>\n",
       "<polygon fill=\"none\" points=\"110.5,-332.5 110.5,-378.5 344.5,-378.5 344.5,-332.5 110.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150.5\" y=\"-351.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"190.5,-332.5 190.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"190.5,-355.5 246.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"246.5,-332.5 246.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-363.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"246.5,-355.5 344.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-340.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 2680433553032&#45;&gt;2680738132872 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2680433553032-&gt;2680738132872</title>\n",
       "<path d=\"M147.558,-415.366C160.411,-405.902 175.571,-394.739 189.214,-384.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"191.501,-387.355 197.478,-378.607 187.351,-381.718 191.501,-387.355\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680433553096 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2680433553096</title>\n",
       "<polygon fill=\"none\" points=\"237,-415.5 237,-461.5 440,-461.5 440,-415.5 237,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-434.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"314,-415.5 314,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"342\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"314,-438.5 370,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"342\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"370,-415.5 370,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405\" y=\"-446.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"370,-438.5 440,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"405\" y=\"-423.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2680433553096&#45;&gt;2680738132872 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2680433553096-&gt;2680738132872</title>\n",
       "<path d=\"M308.169,-415.366C295.198,-405.902 279.901,-394.739 266.134,-384.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"267.936,-381.674 257.795,-378.607 263.809,-387.329 267.936,-381.674\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680738135432 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2680738135432</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 219,-295.5 219,-249.5 0,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32.5\" y=\"-268.8\">Reshape</text>\n",
       "<polyline fill=\"none\" points=\"65,-249.5 65,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"65,-272.5 121,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"121,-249.5 121,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170\" y=\"-280.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"121,-272.5 219,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170\" y=\"-257.3\">(None, 100, 1)</text>\n",
       "</g>\n",
       "<!-- 2680738132872&#45;&gt;2680738135432 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2680738132872-&gt;2680738135432</title>\n",
       "<path d=\"M195.256,-332.366C181.338,-322.812 164.899,-311.528 150.157,-301.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"151.93,-298.381 141.705,-295.607 147.969,-304.152 151.93,-298.381\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680738135368 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2680738135368</title>\n",
       "<polygon fill=\"none\" points=\"237,-249.5 237,-295.5 456,-295.5 456,-249.5 237,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"269.5\" y=\"-268.8\">Reshape</text>\n",
       "<polyline fill=\"none\" points=\"302,-249.5 302,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"302,-272.5 358,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"358,-249.5 358,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"407\" y=\"-280.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"358,-272.5 456,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"407\" y=\"-257.3\">(None, 100, 1)</text>\n",
       "</g>\n",
       "<!-- 2680738132872&#45;&gt;2680738135368 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2680738132872-&gt;2680738135368</title>\n",
       "<path d=\"M260.017,-332.366C274.054,-322.812 290.632,-311.528 305.498,-301.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"307.725,-304.127 314.022,-295.607 303.786,-298.34 307.725,-304.127\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680738134920 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2680738134920</title>\n",
       "<polygon fill=\"none\" points=\"83,-166.5 83,-212.5 372,-212.5 372,-166.5 83,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102\" y=\"-185.8\">Dot</text>\n",
       "<polyline fill=\"none\" points=\"121,-166.5 121,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"121,-189.5 177,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"177,-166.5 177,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-197.3\">[(None, 100, 1), (None, 100, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"177,-189.5 372,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-174.3\">(None, 1, 1)</text>\n",
       "</g>\n",
       "<!-- 2680738135432&#45;&gt;2680738134920 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2680738135432-&gt;2680738134920</title>\n",
       "<path d=\"M141.744,-249.366C155.662,-239.812 172.101,-228.528 186.843,-218.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"189.031,-221.152 195.295,-212.607 185.07,-215.381 189.031,-221.152\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680738135368&#45;&gt;2680738134920 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2680738135368-&gt;2680738134920</title>\n",
       "<path d=\"M313.983,-249.366C299.946,-239.812 283.368,-228.528 268.502,-218.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"270.214,-215.34 259.978,-212.607 266.275,-221.127 270.214,-215.34\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680738117960 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2680738117960</title>\n",
       "<polygon fill=\"none\" points=\"125,-83.5 125,-129.5 330,-129.5 330,-83.5 125,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"157.5\" y=\"-102.8\">Reshape</text>\n",
       "<polyline fill=\"none\" points=\"190,-83.5 190,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"190,-106.5 246,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"246,-83.5 246,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288\" y=\"-114.3\">(None, 1, 1)</text>\n",
       "<polyline fill=\"none\" points=\"246,-106.5 330,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288\" y=\"-91.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2680738134920&#45;&gt;2680738117960 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2680738134920-&gt;2680738117960</title>\n",
       "<path d=\"M227.5,-166.366C227.5,-158.152 227.5,-148.658 227.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"231,-139.607 227.5,-129.607 224,-139.607 231,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2680738117128 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2680738117128</title>\n",
       "<polygon fill=\"none\" points=\"139,-0.5 139,-46.5 316,-46.5 316,-0.5 139,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164.5\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"190,-0.5 190,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"190,-23.5 246,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"246,-0.5 246,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281\" y=\"-31.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"246,-23.5 316,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 2680738117960&#45;&gt;2680738117128 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2680738117960-&gt;2680738117128</title>\n",
       "<path d=\"M227.5,-83.3664C227.5,-75.1516 227.5,-65.6579 227.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"231,-56.6068 227.5,-46.6068 224,-56.6069 231,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 (skip_first, skip_second, relevance) pairs\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-920dcedf2a92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskip_grams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mpair_first_elem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mpair_second_elem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_layer = model.layers[0]\n",
    "#word_model = merge_layer.layers[0]\n",
    "#word_embed_layer = word_model.layers[0]\n",
    "#weights = word_embed_layer.get_weights()[0][1:]\n",
    "word_embed_layer = model.layers[2]\n",
    "weights = word_embed_layer.get_weights()[0][1:]\n",
    "\n",
    "print(weights.shape)\n",
    "pd.DataFrame(weights, index=id2word.values()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['hate','love','nigger','faggot','bitch','pussy','cracker','nigga','homo','cunt','fuck','trash','queer']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "words_ids = [word2id[w] for w in words]\n",
    "word_vectors = np.array([weights[idx] for idx in words_ids])\n",
    "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_vectors)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import nltk\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)\n",
    "\n",
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['hate','love','nigger','faggot','bitch','pussy','cracker','nigga','homo','cunt','fuck','trash','queer']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in corpus]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 10    # Word vector dimensionality  \n",
    "window_context = 10          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sample=sample, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = w2v_model.wv.index2word\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n",
    "                                             num_features=feature_size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(w2v_feature_array)\n",
    "cluster_labels = ap.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([df[['tweet','class']], cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pcs = pca.fit_transform(w2v_feature_array)\n",
    "labels = ap.labels_\n",
    "categories = list(df['class'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    color = 'orange' if label == 0 else 'blue' if label == 1 else 'green'\n",
    "    annotation_label = categories[i]\n",
    "    x, y = pcs[i]\n",
    "    plt.scatter(x, y, c=color, edgecolors='k')\n",
    "    plt.annotate(annotation_label, xy=(x+1e-4, y+1e-3), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "total_vectors = len(nlp.vocab.vectors)\n",
    "print('Total word vectors:', total_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set([word for sublist in [doc.split() for doc in corpus] for word in sublist]))\n",
    "\n",
    "word_glove_vectors = np.array([nlp(word).vector for word in unique_words])\n",
    "pd.DataFrame(word_glove_vectors, index=unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(word_glove_vectors)\n",
    "labels = unique_words\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "doc_glove_vectors = np.array(np.vectorize([nlp(str(doc)).vector for doc in corpus]))\n",
    "\n",
    "km = KMeans(n_clusters=2, random_state=0)\n",
    "km.fit_transform(doc_glove_vectors)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([df[['tweet','class']], cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
